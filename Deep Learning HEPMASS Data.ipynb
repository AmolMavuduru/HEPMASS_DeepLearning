{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for High Energy Particle Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('all_train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># label</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.346368</td>\n",
       "      <td>0.416306</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.427493</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.989833</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>1.566297</td>\n",
       "      <td>...</td>\n",
       "      <td>4.105282</td>\n",
       "      <td>0.267826</td>\n",
       "      <td>0.378718</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>3.406367</td>\n",
       "      <td>4.350537</td>\n",
       "      <td>-0.352571</td>\n",
       "      <td>1.130032</td>\n",
       "      <td>2.227706</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.708236</td>\n",
       "      <td>-0.319394</td>\n",
       "      <td>-1.241873</td>\n",
       "      <td>-0.887231</td>\n",
       "      <td>-0.871906</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-1.038225</td>\n",
       "      <td>0.655748</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.178141</td>\n",
       "      <td>-0.877361</td>\n",
       "      <td>-1.483769</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-1.693781</td>\n",
       "      <td>-0.545062</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>-0.662942</td>\n",
       "      <td>-0.193019</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.360693</td>\n",
       "      <td>1.794174</td>\n",
       "      <td>0.264738</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>-0.292344</td>\n",
       "      <td>-1.054221</td>\n",
       "      <td>-1.150495</td>\n",
       "      <td>1.423404</td>\n",
       "      <td>1.270098</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.199511</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>-1.590629</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.543636</td>\n",
       "      <td>-0.937456</td>\n",
       "      <td>-0.300344</td>\n",
       "      <td>-0.523262</td>\n",
       "      <td>-1.506304</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377914</td>\n",
       "      <td>-0.103932</td>\n",
       "      <td>-0.649434</td>\n",
       "      <td>-2.125015</td>\n",
       "      <td>-1.643797</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>1.011112</td>\n",
       "      <td>-1.040340</td>\n",
       "      <td>-0.541991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463763</td>\n",
       "      <td>-0.006583</td>\n",
       "      <td>1.089122</td>\n",
       "      <td>-0.573682</td>\n",
       "      <td>-0.276348</td>\n",
       "      <td>-0.409272</td>\n",
       "      <td>-0.349926</td>\n",
       "      <td>-0.307123</td>\n",
       "      <td>0.529698</td>\n",
       "      <td>1250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067436</td>\n",
       "      <td>-0.636762</td>\n",
       "      <td>-0.620166</td>\n",
       "      <td>-0.062551</td>\n",
       "      <td>1.588715</td>\n",
       "      <td>-0.005984</td>\n",
       "      <td>-0.595304</td>\n",
       "      <td>-1.238987</td>\n",
       "      <td>0.336844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.552837</td>\n",
       "      <td>-1.418494</td>\n",
       "      <td>-0.562982</td>\n",
       "      <td>1.743123</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>1.560950</td>\n",
       "      <td>-0.150760</td>\n",
       "      <td>-1.023889</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # label        f0        f1        f2        f3        f4        f5  \\\n",
       "0      1.0 -0.346368  0.416306  0.999236  0.475342  0.427493 -0.005984   \n",
       "1      1.0  1.708236 -0.319394 -1.241873 -0.887231 -0.871906 -0.005984   \n",
       "2      0.0 -0.360693  1.794174  0.264738 -0.472273 -0.292344 -1.054221   \n",
       "3      1.0 -0.377914 -0.103932 -0.649434 -2.125015 -1.643797 -0.005984   \n",
       "4      0.0 -0.067436 -0.636762 -0.620166 -0.062551  1.588715 -0.005984   \n",
       "\n",
       "         f6        f7        f8   ...         f18       f19       f20  \\\n",
       "0  1.989833  0.344530  1.566297   ...    4.105282  0.267826  0.378718   \n",
       "1 -0.001047 -1.038225  0.655748   ...   -1.178141 -0.877361 -1.483769   \n",
       "2 -1.150495  1.423404  1.270098   ...   -1.199511  0.539020 -1.590629   \n",
       "3  1.011112 -1.040340 -0.541991   ...    0.463763 -0.006583  1.089122   \n",
       "4 -0.595304 -1.238987  0.336844   ...   -0.552837 -1.418494 -0.562982   \n",
       "\n",
       "        f21       f22       f23       f24       f25       f26    mass  \n",
       "0  1.743123  3.406367  4.350537 -0.352571  1.130032  2.227706  1000.0  \n",
       "1 -0.573682 -1.693781 -0.545062 -0.299118 -0.662942 -0.193019   750.0  \n",
       "2 -0.573682 -0.543636 -0.937456 -0.300344 -0.523262 -1.506304   750.0  \n",
       "3 -0.573682 -0.276348 -0.409272 -0.349926 -0.307123  0.529698  1250.0  \n",
       "4  1.743123  0.881802  0.002516  1.560950 -0.150760 -1.023889   750.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train['mass'] = scaler.fit_transform(train['mass'].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = train.drop(['# label'], axis=1)\n",
    "y = train['# label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5600000 entries, 2283322 to 6413414\n",
      "Data columns (total 28 columns):\n",
      "f0      float64\n",
      "f1      float64\n",
      "f2      float64\n",
      "f3      float64\n",
      "f4      float64\n",
      "f5      float64\n",
      "f6      float64\n",
      "f7      float64\n",
      "f8      float64\n",
      "f9      float64\n",
      "f10     float64\n",
      "f11     float64\n",
      "f12     float64\n",
      "f13     float64\n",
      "f14     float64\n",
      "f15     float64\n",
      "f16     float64\n",
      "f17     float64\n",
      "f18     float64\n",
      "f19     float64\n",
      "f20     float64\n",
      "f21     float64\n",
      "f22     float64\n",
      "f23     float64\n",
      "f24     float64\n",
      "f25     float64\n",
      "f26     float64\n",
      "mass    float64\n",
      "dtypes: float64(28)\n",
      "memory usage: 1.2 GB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training Deep Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2 # L2 regularization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import *\n",
    "\n",
    "def create_deep_neural_net(num_inputs, hidden_layer_sizes, l2_val, num_outputs, optimizer):\n",
    "    \n",
    "    model = Sequential()\n",
    "    first = True\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        if first:\n",
    "            model.add(Dense(hidden_layer_size, \n",
    "                        activation='sigmoid', \n",
    "                        input_dim=num_inputs, kernel_regularizer=l2(l2_val)))\n",
    "            first = False\n",
    "        else:\n",
    "            model.add(Dense(hidden_layer_size, \n",
    "                        activation='sigmoid', \n",
    "                        kernel_regularizer=l2(l2_val)))\n",
    "        \n",
    "    model.add(Dense(num_outputs, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # compiling model\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Width Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 4,089\n",
      "Trainable params: 4,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5600000 samples, validate on 1400000 samples\n",
      "Epoch 1/50\n",
      "5600000/5600000 [==============================] - 62s - loss: 0.3364 - acc: 0.8318 - val_loss: 0.2906 - val_acc: 0.8641\n",
      "Epoch 2/50\n",
      "5600000/5600000 [==============================] - 62s - loss: 0.2885 - acc: 0.8654 - val_loss: 0.2866 - val_acc: 0.8667\n",
      "Epoch 3/50\n",
      "5600000/5600000 [==============================] - 63s - loss: 0.2842 - acc: 0.8683 - val_loss: 0.2819 - val_acc: 0.8704\n",
      "Epoch 4/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2811 - acc: 0.8708 - val_loss: 0.2787 - val_acc: 0.8727\n",
      "Epoch 5/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2786 - acc: 0.8724 - val_loss: 0.2776 - val_acc: 0.8731\n",
      "Epoch 6/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2771 - acc: 0.8734 - val_loss: 0.2755 - val_acc: 0.8744\n",
      "Epoch 7/50\n",
      "5600000/5600000 [==============================] - 63s - loss: 0.2762 - acc: 0.8741 - val_loss: 0.2750 - val_acc: 0.8749\n",
      "Epoch 8/50\n",
      "5600000/5600000 [==============================] - 63s - loss: 0.2754 - acc: 0.8745 - val_loss: 0.2746 - val_acc: 0.8753\n",
      "Epoch 9/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2748 - acc: 0.8749 - val_loss: 0.2746 - val_acc: 0.8752\n",
      "Epoch 10/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2744 - acc: 0.8752 - val_loss: 0.2755 - val_acc: 0.8743\n",
      "Epoch 11/50\n",
      "5600000/5600000 [==============================] - 67s - loss: 0.2739 - acc: 0.8755 - val_loss: 0.2734 - val_acc: 0.8760\n",
      "Epoch 12/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2735 - acc: 0.8757 - val_loss: 0.2741 - val_acc: 0.8754\n",
      "Epoch 13/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2732 - acc: 0.8760 - val_loss: 0.2725 - val_acc: 0.8764\n",
      "Epoch 14/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2728 - acc: 0.8761 - val_loss: 0.2725 - val_acc: 0.8765\n",
      "Epoch 15/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2726 - acc: 0.8763 - val_loss: 0.2719 - val_acc: 0.8768\n",
      "Epoch 16/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2723 - acc: 0.8764 - val_loss: 0.2719 - val_acc: 0.8768\n",
      "Epoch 17/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2721 - acc: 0.8764 - val_loss: 0.2713 - val_acc: 0.8770\n",
      "Epoch 18/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2719 - acc: 0.8766 - val_loss: 0.2716 - val_acc: 0.8769\n",
      "Epoch 19/50\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2717 - acc: 0.8767 - val_loss: 0.2720 - val_acc: 0.8768\n",
      "Epoch 20/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2714 - acc: 0.8768 - val_loss: 0.2709 - val_acc: 0.8773\n",
      "Epoch 21/50\n",
      "5600000/5600000 [==============================] - 67s - loss: 0.2712 - acc: 0.8770 - val_loss: 0.2708 - val_acc: 0.8773\n",
      "Epoch 22/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2711 - acc: 0.8771 - val_loss: 0.2709 - val_acc: 0.8771\n",
      "Epoch 23/50\n",
      "5600000/5600000 [==============================] - 67s - loss: 0.2709 - acc: 0.8772 - val_loss: 0.2718 - val_acc: 0.8767\n",
      "Epoch 24/50\n",
      "5600000/5600000 [==============================] - 66s - loss: 0.2708 - acc: 0.8772 - val_loss: 0.2703 - val_acc: 0.8777\n",
      "Epoch 25/50\n",
      "5600000/5600000 [==============================] - 70s - loss: 0.2707 - acc: 0.8774 - val_loss: 0.2707 - val_acc: 0.8775\n",
      "Epoch 26/50\n",
      "5600000/5600000 [==============================] - 70s - loss: 0.2706 - acc: 0.8775 - val_loss: 0.2707 - val_acc: 0.8772\n",
      "Epoch 27/50\n",
      "5600000/5600000 [==============================] - 67s - loss: 0.2705 - acc: 0.8774 - val_loss: 0.2702 - val_acc: 0.8776\n",
      "Epoch 28/50\n",
      "5600000/5600000 [==============================] - 63s - loss: 0.2703 - acc: 0.8775 - val_loss: 0.2705 - val_acc: 0.8774\n",
      "Epoch 29/50\n",
      "5600000/5600000 [==============================] - 64s - loss: 0.2702 - acc: 0.8776 - val_loss: 0.2699 - val_acc: 0.8778\n",
      "Epoch 30/50\n",
      "5600000/5600000 [==============================] - 62s - loss: 0.2701 - acc: 0.8776 - val_loss: 0.2700 - val_acc: 0.8779\n",
      "Epoch 31/50\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2700 - acc: 0.8778 - val_loss: 0.2697 - val_acc: 0.8779\n",
      "Epoch 32/50\n",
      "5600000/5600000 [==============================] - 60s - loss: 0.2699 - acc: 0.8778 - val_loss: 0.2695 - val_acc: 0.8779\n",
      "Epoch 33/50\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2698 - acc: 0.8777 - val_loss: 0.2695 - val_acc: 0.8780\n",
      "Epoch 34/50\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2697 - acc: 0.8779 - val_loss: 0.2695 - val_acc: 0.8781\n",
      "Epoch 35/50\n",
      "5600000/5600000 [==============================] - 61s - loss: 0.2697 - acc: 0.8779 - val_loss: 0.2690 - val_acc: 0.8782\n",
      "Epoch 36/50\n",
      "5600000/5600000 [==============================] - 60s - loss: 0.2696 - acc: 0.8780 - val_loss: 0.2692 - val_acc: 0.8782\n",
      "Epoch 37/50\n",
      "5600000/5600000 [==============================] - 73s - loss: 0.2695 - acc: 0.8780 - val_loss: 0.2688 - val_acc: 0.8784\n",
      "Epoch 38/50\n",
      "5600000/5600000 [==============================] - 73s - loss: 0.2694 - acc: 0.8781 - val_loss: 0.2700 - val_acc: 0.8776\n",
      "Epoch 39/50\n",
      "5600000/5600000 [==============================] - 72s - loss: 0.2693 - acc: 0.8781 - val_loss: 0.2693 - val_acc: 0.8782\n",
      "Epoch 40/50\n",
      "5596288/5600000 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.8782\n",
      "Epoch 00039: reducing learning rate to 0.0009999999776482583.\n",
      "5600000/5600000 [==============================] - 65s - loss: 0.2693 - acc: 0.8782 - val_loss: 0.2688 - val_acc: 0.8785\n",
      "Epoch 41/50\n",
      "5600000/5600000 [==============================] - 69s - loss: 0.2679 - acc: 0.8790 - val_loss: 0.2678 - val_acc: 0.8789\n",
      "Epoch 42/50\n",
      "5600000/5600000 [==============================] - 70s - loss: 0.2678 - acc: 0.8790 - val_loss: 0.2677 - val_acc: 0.8790\n",
      "Epoch 43/50\n",
      "5600000/5600000 [==============================] - 73s - loss: 0.2677 - acc: 0.8790 - val_loss: 0.2677 - val_acc: 0.8790\n",
      "Epoch 44/50\n",
      "5600000/5600000 [==============================] - 62s - loss: 0.2677 - acc: 0.8791 - val_loss: 0.2677 - val_acc: 0.8790\n",
      "Epoch 45/50\n",
      "5599616/5600000 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.8790\n",
      "Epoch 00044: reducing learning rate to 9.999999310821295e-05.\n",
      "5600000/5600000 [==============================] - 64s - loss: 0.2677 - acc: 0.8790 - val_loss: 0.2677 - val_acc: 0.8789\n",
      "Epoch 46/50\n",
      "5600000/5600000 [==============================] - 62s - loss: 0.2675 - acc: 0.8791 - val_loss: 0.2675 - val_acc: 0.8790\n",
      "Epoch 47/50\n",
      "5600000/5600000 [==============================] - 61s - loss: 0.2675 - acc: 0.8791 - val_loss: 0.2675 - val_acc: 0.8790\n",
      "Epoch 48/50\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2674 - acc: 0.8791 - val_loss: 0.2675 - val_acc: 0.8790\n",
      "Epoch 49/50\n",
      "5596032/5600000 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.8791\n",
      "Epoch 00048: reducing learning rate to 1e-05.\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2674 - acc: 0.8791 - val_loss: 0.2675 - val_acc: 0.8790\n",
      "Epoch 50/50\n",
      "5600000/5600000 [==============================] - 58s - loss: 0.2674 - acc: 0.8791 - val_loss: 0.2675 - val_acc: 0.8790\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2 # L2 regularization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=1, min_lr=0, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='min')\n",
    "\n",
    "L2_CONSTANT = 1e-7 # L2 regularization constant\n",
    "layers = [28] * 5 # 5 layers of size 28\n",
    "\n",
    "sgd = SGD(lr=0.01, momentum=0.95)\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping]\n",
    "\n",
    "\n",
    "uniform_width_network = create_deep_neural_net(num_inputs=28, \n",
    "                                      hidden_layer_sizes=layers,\n",
    "                                      l2_val = L2_CONSTANT,\n",
    "                                      num_outputs=1,\n",
    "                                      optimizer=sgd)\n",
    "\n",
    "print(uniform_width_network.summary())\n",
    "uniform_width_history = uniform_width_network.fit(X_train.values, y_train.values, \n",
    "                 validation_data=(X_valid.values, y_valid.values), callbacks=callbacks, epochs=50, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8701766666666667, 0.8738790476190477, 0.8755861904761905]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_width_history.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning The Number of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting neural network with 1 hidden layer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 841\n",
      "Trainable params: 841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 16s - loss: 0.3472 - acc: 0.8398 - val_loss: 0.3095 - val_acc: 0.8566\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 16s - loss: 0.3022 - acc: 0.8593 - val_loss: 0.2963 - val_acc: 0.8622\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2948 - acc: 0.8627 - val_loss: 0.2919 - val_acc: 0.8644\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 14s - loss: 0.2915 - acc: 0.8645 - val_loss: 0.2895 - val_acc: 0.8657\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 14s - loss: 0.2894 - acc: 0.8658 - val_loss: 0.2878 - val_acc: 0.8667\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2880 - acc: 0.8666 - val_loss: 0.2865 - val_acc: 0.8676\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2869 - acc: 0.8673 - val_loss: 0.2855 - val_acc: 0.8682\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2859 - acc: 0.8680 - val_loss: 0.2845 - val_acc: 0.8689\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2850 - acc: 0.8685 - val_loss: 0.2838 - val_acc: 0.8693\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2843 - acc: 0.8690 - val_loss: 0.2832 - val_acc: 0.8697\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2838 - acc: 0.8693 - val_loss: 0.2826 - val_acc: 0.8699\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2832 - acc: 0.8697 - val_loss: 0.2822 - val_acc: 0.8704\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2828 - acc: 0.8700 - val_loss: 0.2816 - val_acc: 0.8706\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2824 - acc: 0.8702 - val_loss: 0.2816 - val_acc: 0.8705\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2821 - acc: 0.8704 - val_loss: 0.2811 - val_acc: 0.8710\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2818 - acc: 0.8706 - val_loss: 0.2807 - val_acc: 0.8711\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2816 - acc: 0.8708 - val_loss: 0.2808 - val_acc: 0.8713\n",
      "Epoch 18/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2814 - acc: 0.8709 - val_loss: 0.2805 - val_acc: 0.8715\n",
      "Epoch 19/50\n",
      "4900000/4900000 [==============================] - 16s - loss: 0.2812 - acc: 0.8710 - val_loss: 0.2802 - val_acc: 0.8717\n",
      "Epoch 20/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2810 - acc: 0.8712 - val_loss: 0.2803 - val_acc: 0.8716\n",
      "Epoch 21/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2809 - acc: 0.8713 - val_loss: 0.2802 - val_acc: 0.8715\n",
      "Fitting neural network with 2 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 1,653\n",
      "Trainable params: 1,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.3370 - acc: 0.8440 - val_loss: 0.2974 - val_acc: 0.8613\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2925 - acc: 0.8637 - val_loss: 0.2878 - val_acc: 0.8662\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2865 - acc: 0.8670 - val_loss: 0.2838 - val_acc: 0.8687\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2835 - acc: 0.8689 - val_loss: 0.2815 - val_acc: 0.8704\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2814 - acc: 0.8704 - val_loss: 0.2800 - val_acc: 0.8715\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2801 - acc: 0.8713 - val_loss: 0.2788 - val_acc: 0.8721\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2792 - acc: 0.8720 - val_loss: 0.2777 - val_acc: 0.8727\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2785 - acc: 0.8725 - val_loss: 0.2773 - val_acc: 0.8731\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2779 - acc: 0.8728 - val_loss: 0.2769 - val_acc: 0.8735\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2774 - acc: 0.8731 - val_loss: 0.2761 - val_acc: 0.8740\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2770 - acc: 0.8734 - val_loss: 0.2763 - val_acc: 0.8735\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2766 - acc: 0.8736 - val_loss: 0.2757 - val_acc: 0.8741\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2762 - acc: 0.8739 - val_loss: 0.2753 - val_acc: 0.8744\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2759 - acc: 0.8741 - val_loss: 0.2748 - val_acc: 0.8747\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2756 - acc: 0.8744 - val_loss: 0.2743 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2753 - acc: 0.8745 - val_loss: 0.2744 - val_acc: 0.8749\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 17s - loss: 0.2750 - acc: 0.8747 - val_loss: 0.2749 - val_acc: 0.8750\n",
      "Fitting neural network with 3 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 2,465\n",
      "Trainable params: 2,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.3313 - acc: 0.8456 - val_loss: 0.2962 - val_acc: 0.8616\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 18s - loss: 0.2924 - acc: 0.8634 - val_loss: 0.2876 - val_acc: 0.8662\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2868 - acc: 0.8667 - val_loss: 0.2842 - val_acc: 0.8683\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2844 - acc: 0.8683 - val_loss: 0.2825 - val_acc: 0.8696\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2829 - acc: 0.8693 - val_loss: 0.2810 - val_acc: 0.8705\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2815 - acc: 0.8703 - val_loss: 0.2798 - val_acc: 0.8714\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2805 - acc: 0.8710 - val_loss: 0.2793 - val_acc: 0.8716\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2797 - acc: 0.8716 - val_loss: 0.2781 - val_acc: 0.8726\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 15s - loss: 0.2790 - acc: 0.8721 - val_loss: 0.2775 - val_acc: 0.8729\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2783 - acc: 0.8725 - val_loss: 0.2771 - val_acc: 0.8735\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2779 - acc: 0.8729 - val_loss: 0.2769 - val_acc: 0.8736\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2773 - acc: 0.8733 - val_loss: 0.2771 - val_acc: 0.8737\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 18s - loss: 0.2768 - acc: 0.8737 - val_loss: 0.2761 - val_acc: 0.8739\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2763 - acc: 0.8739 - val_loss: 0.2767 - val_acc: 0.8741\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2759 - acc: 0.8742 - val_loss: 0.2747 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 18s - loss: 0.2756 - acc: 0.8744 - val_loss: 0.2745 - val_acc: 0.8750\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 18s - loss: 0.2753 - acc: 0.8745 - val_loss: 0.2743 - val_acc: 0.8752\n",
      "Epoch 18/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2750 - acc: 0.8748 - val_loss: 0.2739 - val_acc: 0.8756\n",
      "Epoch 19/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2747 - acc: 0.8750 - val_loss: 0.2739 - val_acc: 0.8751\n",
      "Epoch 20/50\n",
      "4900000/4900000 [==============================] - 19s - loss: 0.2745 - acc: 0.8751 - val_loss: 0.2739 - val_acc: 0.8753\n",
      "Fitting neural network with 4 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 3,277\n",
      "Trainable params: 3,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.3298 - acc: 0.8458 - val_loss: 0.2951 - val_acc: 0.8623\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2923 - acc: 0.8633 - val_loss: 0.2884 - val_acc: 0.8653\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 22s - loss: 0.2879 - acc: 0.8655 - val_loss: 0.2855 - val_acc: 0.8672\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2855 - acc: 0.8672 - val_loss: 0.2837 - val_acc: 0.8684\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2838 - acc: 0.8684 - val_loss: 0.2823 - val_acc: 0.8694\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2826 - acc: 0.8692 - val_loss: 0.2809 - val_acc: 0.8703\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2817 - acc: 0.8699 - val_loss: 0.2801 - val_acc: 0.8710\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2808 - acc: 0.8705 - val_loss: 0.2805 - val_acc: 0.8706\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2800 - acc: 0.8712 - val_loss: 0.2779 - val_acc: 0.8726\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2792 - acc: 0.8718 - val_loss: 0.2775 - val_acc: 0.8728\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2785 - acc: 0.8723 - val_loss: 0.2809 - val_acc: 0.8701\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2779 - acc: 0.8727 - val_loss: 0.2765 - val_acc: 0.8737\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2774 - acc: 0.8731 - val_loss: 0.2768 - val_acc: 0.8731\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 20s - loss: 0.2770 - acc: 0.8735 - val_loss: 0.2768 - val_acc: 0.8738\n",
      "Fitting neural network with 5 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 4,089\n",
      "Trainable params: 4,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 22s - loss: 0.3271 - acc: 0.8476 - val_loss: 0.2948 - val_acc: 0.8629\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 22s - loss: 0.2917 - acc: 0.8638 - val_loss: 0.2883 - val_acc: 0.8649\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2872 - acc: 0.8663 - val_loss: 0.2843 - val_acc: 0.8683\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2850 - acc: 0.8679 - val_loss: 0.2834 - val_acc: 0.8687\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 22s - loss: 0.2835 - acc: 0.8689 - val_loss: 0.2814 - val_acc: 0.8704\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2824 - acc: 0.8697 - val_loss: 0.2802 - val_acc: 0.8708\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 22s - loss: 0.2815 - acc: 0.8703 - val_loss: 0.2800 - val_acc: 0.8712\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2808 - acc: 0.8708 - val_loss: 0.2794 - val_acc: 0.8717\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2803 - acc: 0.8711 - val_loss: 0.2793 - val_acc: 0.8719\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2797 - acc: 0.8716 - val_loss: 0.2791 - val_acc: 0.8724\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2792 - acc: 0.8721 - val_loss: 0.2774 - val_acc: 0.8730\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2786 - acc: 0.8725 - val_loss: 0.2773 - val_acc: 0.8731\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2782 - acc: 0.8728 - val_loss: 0.2783 - val_acc: 0.8716\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 21s - loss: 0.2777 - acc: 0.8732 - val_loss: 0.2775 - val_acc: 0.8732\n",
      "Fitting neural network with 6 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 4,901\n",
      "Trainable params: 4,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.3365 - acc: 0.8448 - val_loss: 0.3005 - val_acc: 0.8609\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2967 - acc: 0.8616 - val_loss: 0.2913 - val_acc: 0.8648\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2915 - acc: 0.8638 - val_loss: 0.2911 - val_acc: 0.8644\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2892 - acc: 0.8650 - val_loss: 0.2864 - val_acc: 0.8668\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2875 - acc: 0.8659 - val_loss: 0.2892 - val_acc: 0.8648\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2864 - acc: 0.8665 - val_loss: 0.2844 - val_acc: 0.8678\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2853 - acc: 0.8671 - val_loss: 0.2832 - val_acc: 0.8687\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2846 - acc: 0.8677 - val_loss: 0.2825 - val_acc: 0.8692\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2838 - acc: 0.8683 - val_loss: 0.2820 - val_acc: 0.8693\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2831 - acc: 0.8687 - val_loss: 0.2819 - val_acc: 0.8697\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2824 - acc: 0.8694 - val_loss: 0.2806 - val_acc: 0.8706\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2818 - acc: 0.8698 - val_loss: 0.2848 - val_acc: 0.8676\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2813 - acc: 0.8703 - val_loss: 0.2804 - val_acc: 0.8704\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2809 - acc: 0.8706 - val_loss: 0.2794 - val_acc: 0.8716\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2803 - acc: 0.8710 - val_loss: 0.2791 - val_acc: 0.8719\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2800 - acc: 0.8712 - val_loss: 0.2783 - val_acc: 0.8725\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2797 - acc: 0.8715 - val_loss: 0.2798 - val_acc: 0.8707\n",
      "Epoch 18/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2795 - acc: 0.8716 - val_loss: 0.2780 - val_acc: 0.8726\n",
      "Epoch 19/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2791 - acc: 0.8719 - val_loss: 0.2786 - val_acc: 0.8723\n",
      "Epoch 20/50\n",
      "4900000/4900000 [==============================] - 23s - loss: 0.2790 - acc: 0.8720 - val_loss: 0.2775 - val_acc: 0.8727\n",
      "Epoch 21/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2787 - acc: 0.8723 - val_loss: 0.2786 - val_acc: 0.8724\n",
      "Epoch 22/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2785 - acc: 0.8723 - val_loss: 0.2781 - val_acc: 0.8727\n",
      "Fitting neural network with 7 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 5,713\n",
      "Trainable params: 5,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.3356 - acc: 0.8412 - val_loss: 0.3058 - val_acc: 0.8519\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2940 - acc: 0.8625 - val_loss: 0.2906 - val_acc: 0.8647\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2894 - acc: 0.8645 - val_loss: 0.2865 - val_acc: 0.8662\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2873 - acc: 0.8656 - val_loss: 0.2846 - val_acc: 0.8673\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2853 - acc: 0.8669 - val_loss: 0.2840 - val_acc: 0.8675\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2842 - acc: 0.8678 - val_loss: 0.2829 - val_acc: 0.8685\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2831 - acc: 0.8686 - val_loss: 0.2830 - val_acc: 0.8690\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2822 - acc: 0.8693 - val_loss: 0.2799 - val_acc: 0.8707\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2814 - acc: 0.8699 - val_loss: 0.2799 - val_acc: 0.8709\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2805 - acc: 0.8708 - val_loss: 0.2796 - val_acc: 0.8706\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2799 - acc: 0.8712 - val_loss: 0.2781 - val_acc: 0.8723\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 25s - loss: 0.2791 - acc: 0.8717 - val_loss: 0.2778 - val_acc: 0.8728\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 7224s - loss: 0.2787 - acc: 0.8722 - val_loss: 0.2779 - val_acc: 0.8729\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 1831s - loss: 0.2780 - acc: 0.8726 - val_loss: 0.2771 - val_acc: 0.8732\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 35s - loss: 0.2775 - acc: 0.8731 - val_loss: 0.2760 - val_acc: 0.8743\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 3630s - loss: 0.2771 - acc: 0.8734 - val_loss: 0.2756 - val_acc: 0.8742\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 7233s - loss: 0.2765 - acc: 0.8737 - val_loss: 0.2757 - val_acc: 0.8739\n",
      "Epoch 18/50\n",
      "4900000/4900000 [==============================] - 34s - loss: 0.2761 - acc: 0.8740 - val_loss: 0.2753 - val_acc: 0.8748\n",
      "Epoch 19/50\n",
      "4900000/4900000 [==============================] - 7232s - loss: 0.2756 - acc: 0.8743 - val_loss: 0.2744 - val_acc: 0.8750\n",
      "Epoch 20/50\n",
      "4900000/4900000 [==============================] - 35s - loss: 0.2752 - acc: 0.8746 - val_loss: 0.2747 - val_acc: 0.8752\n",
      "Epoch 21/50\n",
      "4900000/4900000 [==============================] - 7125s - loss: 0.2749 - acc: 0.8748 - val_loss: 0.2735 - val_acc: 0.8760\n",
      "Epoch 22/50\n",
      "4900000/4900000 [==============================] - 24s - loss: 0.2746 - acc: 0.8749 - val_loss: 0.2743 - val_acc: 0.8751\n",
      "Epoch 23/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2743 - acc: 0.8751 - val_loss: 0.2738 - val_acc: 0.8753\n",
      "Fitting neural network with 8 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 6,525\n",
      "Trainable params: 6,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.3423 - acc: 0.8353 - val_loss: 0.3071 - val_acc: 0.8562\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2967 - acc: 0.8608 - val_loss: 0.2940 - val_acc: 0.8614\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2902 - acc: 0.8645 - val_loss: 0.2883 - val_acc: 0.8658\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2871 - acc: 0.8665 - val_loss: 0.2878 - val_acc: 0.8657\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2851 - acc: 0.8678 - val_loss: 0.2893 - val_acc: 0.8662\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2837 - acc: 0.8688 - val_loss: 0.2827 - val_acc: 0.8691\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2825 - acc: 0.8697 - val_loss: 0.2831 - val_acc: 0.8695\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2817 - acc: 0.8704 - val_loss: 0.2799 - val_acc: 0.8715\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2808 - acc: 0.8711 - val_loss: 0.2791 - val_acc: 0.8722\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2805 - acc: 0.8714 - val_loss: 0.2793 - val_acc: 0.8720\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2799 - acc: 0.8718 - val_loss: 0.2791 - val_acc: 0.8721\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2794 - acc: 0.8721 - val_loss: 0.2781 - val_acc: 0.8728\n",
      "Epoch 13/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2789 - acc: 0.8724 - val_loss: 0.2771 - val_acc: 0.8734\n",
      "Epoch 14/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2784 - acc: 0.8728 - val_loss: 0.2779 - val_acc: 0.8731\n",
      "Epoch 15/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2780 - acc: 0.8730 - val_loss: 0.2769 - val_acc: 0.8737\n",
      "Epoch 16/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2776 - acc: 0.8732 - val_loss: 0.2762 - val_acc: 0.8740\n",
      "Epoch 17/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2770 - acc: 0.8737 - val_loss: 0.2771 - val_acc: 0.8736\n",
      "Epoch 18/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2767 - acc: 0.8738 - val_loss: 0.2752 - val_acc: 0.8746\n",
      "Epoch 19/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2764 - acc: 0.8740 - val_loss: 0.2748 - val_acc: 0.8748\n",
      "Epoch 20/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2761 - acc: 0.8743 - val_loss: 0.2751 - val_acc: 0.8745\n",
      "Epoch 21/50\n",
      "4900000/4900000 [==============================] - 26s - loss: 0.2757 - acc: 0.8745 - val_loss: 0.2748 - val_acc: 0.8750\n",
      "Fitting neural network with 9 hidden layers\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 7,337\n",
      "Trainable params: 7,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4900000 samples, validate on 2100000 samples\n",
      "Epoch 1/50\n",
      "4900000/4900000 [==============================] - 28s - loss: 0.3531 - acc: 0.8289 - val_loss: 0.2986 - val_acc: 0.8596\n",
      "Epoch 2/50\n",
      "4900000/4900000 [==============================] - 28s - loss: 0.2941 - acc: 0.8623 - val_loss: 0.2886 - val_acc: 0.8657\n",
      "Epoch 3/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2884 - acc: 0.8658 - val_loss: 0.2863 - val_acc: 0.8669\n",
      "Epoch 4/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2862 - acc: 0.8672 - val_loss: 0.2841 - val_acc: 0.8689\n",
      "Epoch 5/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2845 - acc: 0.8684 - val_loss: 0.2829 - val_acc: 0.8694\n",
      "Epoch 6/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2831 - acc: 0.8694 - val_loss: 0.2816 - val_acc: 0.8703\n",
      "Epoch 7/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2816 - acc: 0.8704 - val_loss: 0.2802 - val_acc: 0.8718\n",
      "Epoch 8/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2806 - acc: 0.8712 - val_loss: 0.2789 - val_acc: 0.8719\n",
      "Epoch 9/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2796 - acc: 0.8720 - val_loss: 0.2779 - val_acc: 0.8734\n",
      "Epoch 10/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2787 - acc: 0.8727 - val_loss: 0.2775 - val_acc: 0.8730\n",
      "Epoch 11/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2781 - acc: 0.8732 - val_loss: 0.2814 - val_acc: 0.8701\n",
      "Epoch 12/50\n",
      "4900000/4900000 [==============================] - 27s - loss: 0.2776 - acc: 0.8735 - val_loss: 0.2786 - val_acc: 0.8729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16585dbe0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnGDMzKwUTjBmZlaK/wcZ/BxFGhaVZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158dc1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=1, mode='min')\n",
    "val_accs = []\n",
    "for num_layers in range(1, 8):\n",
    "    L2_CONSTANT = 1e-7 # L2 regularization constant\n",
    "    layers = [28] * num_layers # 5 layers of size 28\n",
    "\n",
    "    uniform_width_network = create_deep_neural_net(num_inputs=28, \n",
    "                                      hidden_layer_sizes=layers,\n",
    "                                      l2_val = L2_CONSTANT,\n",
    "                                      num_outputs=1,\n",
    "                                      learning_rate = 0.001)\n",
    "    if num_layers == 1:\n",
    "        print('Fitting neural network with 1 hidden layer')\n",
    "    else:\n",
    "        print('Fitting neural network with {} hidden layers'.format(num_layers))\n",
    "    print(uniform_width_network.summary())\n",
    "    uniform_width_history = uniform_width_network.fit(X_train.values, y_train.values, \n",
    "                 validation_data=(X_valid.values, y_valid.values), epochs=20, \n",
    "                                                      batch_size=1024,\n",
    "                                                     callbacks=[early_stopping])\n",
    "    val_accs.append(uniform_width_history.history['val_acc'][-1])\n",
    "\n",
    "plt.title('Validation accuracy with different layer depths')\n",
    "plt.xlabel('Number of hidden layers')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.plot(range(1, 8), val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITM (Wide in the Middle) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_witm(num_inputs, l2_val, width_factor, depth_factor, num_outputs, optimizer):\n",
    "    \n",
    "    layers = []\n",
    "    for i in range(depth_factor+1):\n",
    "        layers.append((width_factor ** i) * num_inputs)\n",
    "    \n",
    "    for i in range(depth_factor):\n",
    "        layers.append(layers[-1] // width_factor)\n",
    "    \n",
    "    print(layers)\n",
    "    witm_network = create_deep_neural_net(num_inputs=num_inputs,\n",
    "                                         hidden_layer_sizes=layers,\n",
    "                                         l2_val=l2_val,\n",
    "                                         num_outputs=num_outputs,\n",
    "                                         optimizer=optimizer)\n",
    "    return witm_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting network with depth factor = 1...\n",
      "[28, 56, 28]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 28)                1596      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 4,061\n",
      "Trainable params: 4,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5600000 samples, validate on 1400000 samples\n",
      "Epoch 1/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.7037 - acc: 0.4564 - val_loss: 0.6959 - val_acc: 0.3910\n",
      "Epoch 2/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6956 - acc: 0.4039 - val_loss: 0.6952 - val_acc: 0.4165\n",
      "Epoch 3/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6949 - acc: 0.4309 - val_loss: 0.6946 - val_acc: 0.4439\n",
      "Epoch 4/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6943 - acc: 0.4590 - val_loss: 0.6940 - val_acc: 0.4723\n",
      "Epoch 5/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6937 - acc: 0.4870 - val_loss: 0.6934 - val_acc: 0.5006\n",
      "Epoch 6/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6931 - acc: 0.5140 - val_loss: 0.6927 - val_acc: 0.5265\n",
      "Epoch 7/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6924 - acc: 0.5394 - val_loss: 0.6921 - val_acc: 0.5502\n",
      "Epoch 8/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6918 - acc: 0.5620 - val_loss: 0.6915 - val_acc: 0.5728\n",
      "Epoch 9/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6911 - acc: 0.5820 - val_loss: 0.6908 - val_acc: 0.5935\n",
      "Epoch 10/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6904 - acc: 0.6006 - val_loss: 0.6901 - val_acc: 0.6095\n",
      "Epoch 11/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6897 - acc: 0.6160 - val_loss: 0.6893 - val_acc: 0.6256\n",
      "Epoch 12/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6889 - acc: 0.6311 - val_loss: 0.6885 - val_acc: 0.6361\n",
      "Epoch 13/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6881 - acc: 0.6432 - val_loss: 0.6876 - val_acc: 0.6494\n",
      "Epoch 14/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6871 - acc: 0.6553 - val_loss: 0.6867 - val_acc: 0.6595\n",
      "Epoch 15/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.6861 - acc: 0.6656 - val_loss: 0.6856 - val_acc: 0.6716\n",
      "Epoch 16/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6850 - acc: 0.6757 - val_loss: 0.6844 - val_acc: 0.6807\n",
      "Epoch 17/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.6838 - acc: 0.6847 - val_loss: 0.6831 - val_acc: 0.6895\n",
      "Epoch 18/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6824 - acc: 0.6931 - val_loss: 0.6816 - val_acc: 0.6975\n",
      "Epoch 19/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6808 - acc: 0.7008 - val_loss: 0.6800 - val_acc: 0.7049\n",
      "Epoch 20/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6791 - acc: 0.7084 - val_loss: 0.6781 - val_acc: 0.7109\n",
      "Epoch 21/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6771 - acc: 0.7150 - val_loss: 0.6759 - val_acc: 0.7173\n",
      "Epoch 22/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6748 - acc: 0.7212 - val_loss: 0.6735 - val_acc: 0.7238\n",
      "Epoch 23/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6721 - acc: 0.7273 - val_loss: 0.6706 - val_acc: 0.7293\n",
      "Epoch 24/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.6690 - acc: 0.7329 - val_loss: 0.6673 - val_acc: 0.7351\n",
      "Epoch 25/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6655 - acc: 0.7382 - val_loss: 0.6635 - val_acc: 0.7402\n",
      "Epoch 26/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6613 - acc: 0.7433 - val_loss: 0.6590 - val_acc: 0.7456\n",
      "Epoch 27/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6565 - acc: 0.7480 - val_loss: 0.6537 - val_acc: 0.7503\n",
      "Epoch 28/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6508 - acc: 0.7526 - val_loss: 0.6476 - val_acc: 0.7543\n",
      "Epoch 29/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6441 - acc: 0.7568 - val_loss: 0.6404 - val_acc: 0.7583\n",
      "Epoch 30/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6364 - acc: 0.7607 - val_loss: 0.6319 - val_acc: 0.7627\n",
      "Epoch 31/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6273 - acc: 0.7645 - val_loss: 0.6221 - val_acc: 0.7665\n",
      "Epoch 32/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6167 - acc: 0.7680 - val_loss: 0.6109 - val_acc: 0.7699\n",
      "Epoch 33/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.6047 - acc: 0.7714 - val_loss: 0.5980 - val_acc: 0.7729\n",
      "Epoch 34/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.5912 - acc: 0.7745 - val_loss: 0.5838 - val_acc: 0.7762\n",
      "Epoch 35/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.5764 - acc: 0.7776 - val_loss: 0.5684 - val_acc: 0.7791\n",
      "Epoch 36/100\n",
      "5600000/5600000 [==============================] - 23s - loss: 0.5606 - acc: 0.7804 - val_loss: 0.5522 - val_acc: 0.7820\n",
      "Epoch 37/100\n",
      "5600000/5600000 [==============================] - 23s - loss: 0.5443 - acc: 0.7831 - val_loss: 0.5359 - val_acc: 0.7847\n",
      "Epoch 38/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.5282 - acc: 0.7857 - val_loss: 0.5201 - val_acc: 0.7873\n",
      "Epoch 39/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.5129 - acc: 0.7882 - val_loss: 0.5053 - val_acc: 0.7897\n",
      "Epoch 40/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.4989 - acc: 0.7906 - val_loss: 0.4920 - val_acc: 0.7920\n",
      "Epoch 41/100\n",
      "5600000/5600000 [==============================] - 34s - loss: 0.4864 - acc: 0.7929 - val_loss: 0.4803 - val_acc: 0.7942\n",
      "Epoch 42/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.4755 - acc: 0.7952 - val_loss: 0.4702 - val_acc: 0.7963\n",
      "Epoch 43/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.4662 - acc: 0.7973 - val_loss: 0.4616 - val_acc: 0.7986\n",
      "Epoch 44/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.4582 - acc: 0.7994 - val_loss: 0.4542 - val_acc: 0.8007\n",
      "Epoch 45/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.4513 - acc: 0.8015 - val_loss: 0.4477 - val_acc: 0.8028\n",
      "Epoch 46/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.4453 - acc: 0.8034 - val_loss: 0.4420 - val_acc: 0.8047\n",
      "Epoch 47/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.4399 - acc: 0.8053 - val_loss: 0.4370 - val_acc: 0.8065\n",
      "Epoch 48/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.4351 - acc: 0.8072 - val_loss: 0.4323 - val_acc: 0.8083\n",
      "Epoch 49/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.4307 - acc: 0.8090 - val_loss: 0.4281 - val_acc: 0.8100\n",
      "Epoch 50/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.4266 - acc: 0.8107 - val_loss: 0.4241 - val_acc: 0.8117\n",
      "Epoch 51/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.4227 - acc: 0.8123 - val_loss: 0.4203 - val_acc: 0.8134\n",
      "Epoch 52/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.4190 - acc: 0.8139 - val_loss: 0.4167 - val_acc: 0.8151\n",
      "Epoch 53/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.4155 - acc: 0.8155 - val_loss: 0.4132 - val_acc: 0.8166\n",
      "Epoch 54/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.4121 - acc: 0.8169 - val_loss: 0.4099 - val_acc: 0.8180\n",
      "Epoch 55/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.4089 - acc: 0.8183 - val_loss: 0.4067 - val_acc: 0.8193\n",
      "Epoch 56/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.4058 - acc: 0.8197 - val_loss: 0.4037 - val_acc: 0.8206\n",
      "Epoch 57/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.4028 - acc: 0.8209 - val_loss: 0.4007 - val_acc: 0.8219\n",
      "Epoch 58/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.3999 - acc: 0.8220 - val_loss: 0.3979 - val_acc: 0.8232\n",
      "Epoch 59/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.3972 - acc: 0.8232 - val_loss: 0.3952 - val_acc: 0.8243\n",
      "Epoch 60/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.3946 - acc: 0.8242 - val_loss: 0.3926 - val_acc: 0.8253\n",
      "Epoch 61/100\n",
      "5600000/5600000 [==============================] - 22s - loss: 0.3921 - acc: 0.8252 - val_loss: 0.3902 - val_acc: 0.8263\n",
      "Epoch 62/100\n",
      "5600000/5600000 [==============================] - 23s - loss: 0.3897 - acc: 0.8261 - val_loss: 0.3879 - val_acc: 0.8271\n",
      "Epoch 63/100\n",
      "5600000/5600000 [==============================] - 677s - loss: 0.3875 - acc: 0.8270 - val_loss: 0.3857 - val_acc: 0.8280\n",
      "Epoch 64/100\n",
      "5600000/5600000 [==============================] - 4863s - loss: 0.3854 - acc: 0.8278 - val_loss: 0.3837 - val_acc: 0.8287\n",
      "Epoch 65/100\n",
      "5600000/5600000 [==============================] - 94s - loss: 0.3835 - acc: 0.8286 - val_loss: 0.3819 - val_acc: 0.8294\n",
      "Epoch 66/100\n",
      "5600000/5600000 [==============================] - 22s - loss: 0.3817 - acc: 0.8293 - val_loss: 0.3801 - val_acc: 0.8301\n",
      "Epoch 67/100\n",
      "5600000/5600000 [==============================] - 22s - loss: 0.3801 - acc: 0.8299 - val_loss: 0.3785 - val_acc: 0.8308\n",
      "Epoch 68/100\n",
      "5600000/5600000 [==============================] - 22s - loss: 0.3786 - acc: 0.8306 - val_loss: 0.3771 - val_acc: 0.8314\n",
      "Epoch 69/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.3772 - acc: 0.8311 - val_loss: 0.3757 - val_acc: 0.8320\n",
      "Epoch 70/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.3759 - acc: 0.8317 - val_loss: 0.3745 - val_acc: 0.8325\n",
      "Epoch 71/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.3748 - acc: 0.8322 - val_loss: 0.3735 - val_acc: 0.8330\n",
      "Epoch 72/100\n",
      "5600000/5600000 [==============================] - 23s - loss: 0.3738 - acc: 0.8326 - val_loss: 0.3725 - val_acc: 0.8335\n",
      "Epoch 73/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.3729 - acc: 0.8331 - val_loss: 0.3716 - val_acc: 0.8339\n",
      "Epoch 74/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.3720 - acc: 0.8334 - val_loss: 0.3708 - val_acc: 0.8343\n",
      "Epoch 75/100\n",
      "5600000/5600000 [==============================] - 24s - loss: 0.3713 - acc: 0.8338 - val_loss: 0.3701 - val_acc: 0.8346\n",
      "Epoch 76/100\n",
      "5600000/5600000 [==============================] - 25s - loss: 0.3707 - acc: 0.8342 - val_loss: 0.3695 - val_acc: 0.8349\n",
      "Epoch 77/100\n",
      "5600000/5600000 [==============================] - 24s - loss: 0.3701 - acc: 0.8345 - val_loss: 0.3690 - val_acc: 0.8351\n",
      "Epoch 78/100\n",
      "5600000/5600000 [==============================] - 24s - loss: 0.3696 - acc: 0.8348 - val_loss: 0.3685 - val_acc: 0.8354\n",
      "Epoch 79/100\n",
      "5600000/5600000 [==============================] - 22s - loss: 0.3691 - acc: 0.8350 - val_loss: 0.3680 - val_acc: 0.8356\n",
      "Epoch 80/100\n",
      "5600000/5600000 [==============================] - 26s - loss: 0.3687 - acc: 0.8352 - val_loss: 0.3676 - val_acc: 0.8357\n",
      "Epoch 81/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.3683 - acc: 0.8354 - val_loss: 0.3673 - val_acc: 0.8360\n",
      "Epoch 82/100\n",
      "5600000/5600000 [==============================] - 228s - loss: 0.3680 - acc: 0.8356 - val_loss: 0.3669 - val_acc: 0.8361\n",
      "Epoch 83/100\n",
      "5600000/5600000 [==============================] - 255s - loss: 0.3677 - acc: 0.8358 - val_loss: 0.3666 - val_acc: 0.8363\n",
      "Epoch 84/100\n",
      "5600000/5600000 [==============================] - 109s - loss: 0.3674 - acc: 0.8359 - val_loss: 0.3664 - val_acc: 0.8364\n",
      "Epoch 85/100\n",
      "5600000/5600000 [==============================] - 67s - loss: 0.3671 - acc: 0.8361 - val_loss: 0.3661 - val_acc: 0.8366\n",
      "Epoch 86/100\n",
      "5600000/5600000 [==============================] - 103s - loss: 0.3669 - acc: 0.8362 - val_loss: 0.3659 - val_acc: 0.8368\n",
      "Epoch 87/100\n",
      "5600000/5600000 [==============================] - 94s - loss: 0.3667 - acc: 0.8363 - val_loss: 0.3657 - val_acc: 0.8369\n",
      "Epoch 88/100\n",
      "5600000/5600000 [==============================] - 38s - loss: 0.3665 - acc: 0.8364 - val_loss: 0.3655 - val_acc: 0.8370\n",
      "Epoch 89/100\n",
      "5600000/5600000 [==============================] - 148s - loss: 0.3663 - acc: 0.8365 - val_loss: 0.3653 - val_acc: 0.8371\n",
      "Epoch 90/100\n",
      "5600000/5600000 [==============================] - 25s - loss: 0.3661 - acc: 0.8366 - val_loss: 0.3651 - val_acc: 0.8372\n",
      "Epoch 91/100\n",
      "5600000/5600000 [==============================] - 25s - loss: 0.3659 - acc: 0.8367 - val_loss: 0.3649 - val_acc: 0.8373\n",
      "Epoch 92/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.3658 - acc: 0.8368 - val_loss: 0.3648 - val_acc: 0.8374\n",
      "Epoch 93/100\n",
      "5600000/5600000 [==============================] - 24s - loss: 0.3656 - acc: 0.8369 - val_loss: 0.3646 - val_acc: 0.8374\n",
      "Epoch 94/100\n",
      "5600000/5600000 [==============================] - 24s - loss: 0.3655 - acc: 0.8370 - val_loss: 0.3645 - val_acc: 0.8375\n",
      "Epoch 95/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.3653 - acc: 0.8371 - val_loss: 0.3644 - val_acc: 0.8376\n",
      "Epoch 96/100\n",
      "5600000/5600000 [==============================] - 28s - loss: 0.3652 - acc: 0.8371 - val_loss: 0.3642 - val_acc: 0.8377\n",
      "Epoch 97/100\n",
      "5600000/5600000 [==============================] - 25s - loss: 0.3651 - acc: 0.8372 - val_loss: 0.3641 - val_acc: 0.8377\n",
      "Epoch 98/100\n",
      "5600000/5600000 [==============================] - 27s - loss: 0.3650 - acc: 0.8372 - val_loss: 0.3640 - val_acc: 0.8377\n",
      "Epoch 99/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.3648 - acc: 0.8373 - val_loss: 0.3639 - val_acc: 0.8378\n",
      "Epoch 100/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.3647 - acc: 0.8373 - val_loss: 0.3637 - val_acc: 0.8378\n"
     ]
    }
   ],
   "source": [
    "L2_CONSTANT = 1e-7 # L2 regularization constant\n",
    "val_accs_witm = []\n",
    "\n",
    "depth_factor=1\n",
    "\n",
    "print('Fitting network with depth factor = {}...'.format(depth_factor))\n",
    "witm_network = create_witm(num_inputs=28,\n",
    "                              l2_val=L2_CONSTANT,\n",
    "                              width_factor=2,\n",
    "                              depth_factor=depth_factor,\n",
    "                              num_outputs=1,\n",
    "                              optimizer=sgd)\n",
    "print(witm_network.summary())\n",
    "    # fitting model\n",
    "history = witm_network.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), epochs=100, \n",
    "                 batch_size=512, callbacks=callbacks)\n",
    "val_accs_witm.append(max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting network with depth factor = 2...\n",
      "[28, 56, 112, 56, 28]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 112)               6384      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 56)                6328      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 28)                1596      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5600000 samples, validate on 1400000 samples\n",
      "Epoch 1/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.3286 - acc: 0.8476 - val_loss: 0.2950 - val_acc: 0.8616\n",
      "Epoch 2/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.2903 - acc: 0.8643 - val_loss: 0.2854 - val_acc: 0.8674\n",
      "Epoch 3/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.2840 - acc: 0.8685 - val_loss: 0.2830 - val_acc: 0.8692\n",
      "Epoch 4/100\n",
      "5600000/5600000 [==============================] - 29s - loss: 0.2808 - acc: 0.8707 - val_loss: 0.2785 - val_acc: 0.8728\n",
      "Epoch 5/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.2787 - acc: 0.8721 - val_loss: 0.2768 - val_acc: 0.8735\n",
      "Epoch 6/100\n",
      "5600000/5600000 [==============================] - 30s - loss: 0.2770 - acc: 0.8733 - val_loss: 0.2786 - val_acc: 0.8726\n",
      "Epoch 7/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.2758 - acc: 0.8741 - val_loss: 0.2752 - val_acc: 0.8746\n",
      "Epoch 8/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.2747 - acc: 0.8748 - val_loss: 0.2732 - val_acc: 0.8761\n",
      "Epoch 9/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.2739 - acc: 0.8753 - val_loss: 0.2728 - val_acc: 0.8765\n",
      "Epoch 10/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2732 - acc: 0.8759 - val_loss: 0.2733 - val_acc: 0.8760\n",
      "Epoch 11/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2726 - acc: 0.8761 - val_loss: 0.2720 - val_acc: 0.8766\n",
      "Epoch 12/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2721 - acc: 0.8766 - val_loss: 0.2709 - val_acc: 0.8775\n",
      "Epoch 13/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2717 - acc: 0.8767 - val_loss: 0.2709 - val_acc: 0.8773\n",
      "Epoch 14/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2713 - acc: 0.8770 - val_loss: 0.2715 - val_acc: 0.8771\n",
      "Epoch 15/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2709 - acc: 0.8773 - val_loss: 0.2700 - val_acc: 0.8777\n",
      "Epoch 16/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2705 - acc: 0.8775 - val_loss: 0.2699 - val_acc: 0.8779\n",
      "Epoch 17/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2701 - acc: 0.8778 - val_loss: 0.2693 - val_acc: 0.8783\n",
      "Epoch 18/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2698 - acc: 0.8780 - val_loss: 0.2689 - val_acc: 0.8785\n",
      "Epoch 19/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2695 - acc: 0.8781 - val_loss: 0.2687 - val_acc: 0.8787\n",
      "Epoch 20/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2691 - acc: 0.8784 - val_loss: 0.2689 - val_acc: 0.8787\n",
      "Epoch 21/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2688 - acc: 0.8785 - val_loss: 0.2681 - val_acc: 0.8791\n",
      "Epoch 22/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2685 - acc: 0.8787 - val_loss: 0.2676 - val_acc: 0.8794\n",
      "Epoch 23/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2683 - acc: 0.8788 - val_loss: 0.2685 - val_acc: 0.8789\n",
      "Epoch 24/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2681 - acc: 0.8789 - val_loss: 0.2679 - val_acc: 0.8794\n",
      "Epoch 25/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2679 - acc: 0.8790 - val_loss: 0.2676 - val_acc: 0.8792\n",
      "Epoch 26/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2677 - acc: 0.8792 - val_loss: 0.2674 - val_acc: 0.8797\n",
      "Epoch 27/100\n",
      "5600000/5600000 [==============================] - 31s - loss: 0.2675 - acc: 0.8792 - val_loss: 0.2670 - val_acc: 0.8796\n",
      "Epoch 28/100\n",
      "5600000/5600000 [==============================] - 32s - loss: 0.2673 - acc: 0.8793 - val_loss: 0.2668 - val_acc: 0.8798\n",
      "Epoch 29/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2672 - acc: 0.8794 - val_loss: 0.2669 - val_acc: 0.8795\n",
      "Epoch 30/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2670 - acc: 0.8795 - val_loss: 0.2669 - val_acc: 0.8798\n",
      "Epoch 31/100\n",
      "5600000/5600000 [==============================] - 33s - loss: 0.2668 - acc: 0.8796 - val_loss: 0.2673 - val_acc: 0.8790\n"
     ]
    }
   ],
   "source": [
    "depth_factor=2\n",
    "\n",
    "print('Fitting network with depth factor = {}...'.format(depth_factor))\n",
    "witm_network = create_witm(num_inputs=28,\n",
    "                              l2_val=L2_CONSTANT,\n",
    "                              width_factor=2,\n",
    "                              depth_factor=depth_factor,\n",
    "                              num_outputs=1,\n",
    "                              optimizer=sgd)\n",
    "print(witm_network.summary())\n",
    "    # fitting model\n",
    "history = witm_network.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), epochs=100, \n",
    "                 batch_size=1024, callbacks=callbacks)\n",
    "val_accs_witm.append(max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting network with depth factor = 3...\n",
      "[28, 56, 112, 224, 112, 56, 28]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 28)                812       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 56)                1624      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 112)               6384      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 224)               25312     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 112)               25200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 56)                6328      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 28)                1596      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 29        \n",
      "=================================================================\n",
      "Total params: 67,285\n",
      "Trainable params: 67,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5600000 samples, validate on 1400000 samples\n",
      "Epoch 1/100\n",
      "5600000/5600000 [==============================] - 52s - loss: 0.3293 - acc: 0.8450 - val_loss: 0.2937 - val_acc: 0.8622\n",
      "Epoch 2/100\n",
      "5600000/5600000 [==============================] - 53s - loss: 0.2903 - acc: 0.8640 - val_loss: 0.2861 - val_acc: 0.8669\n",
      "Epoch 3/100\n",
      "5600000/5600000 [==============================] - 51s - loss: 0.2850 - acc: 0.8675 - val_loss: 0.2825 - val_acc: 0.8692\n",
      "Epoch 4/100\n",
      "5600000/5600000 [==============================] - 49s - loss: 0.2828 - acc: 0.8689 - val_loss: 0.2807 - val_acc: 0.8708\n",
      "Epoch 5/100\n",
      "5600000/5600000 [==============================] - 47s - loss: 0.2810 - acc: 0.8703 - val_loss: 0.2791 - val_acc: 0.8716\n",
      "Epoch 6/100\n",
      "5600000/5600000 [==============================] - 46s - loss: 0.2793 - acc: 0.8715 - val_loss: 0.2779 - val_acc: 0.8728\n",
      "Epoch 7/100\n",
      "5600000/5600000 [==============================] - 46s - loss: 0.2782 - acc: 0.8725 - val_loss: 0.2774 - val_acc: 0.8730\n",
      "Epoch 8/100\n",
      "5600000/5600000 [==============================] - 50s - loss: 0.2775 - acc: 0.8731 - val_loss: 0.2761 - val_acc: 0.8742\n",
      "Epoch 9/100\n",
      "5600000/5600000 [==============================] - 52s - loss: 0.2769 - acc: 0.8735 - val_loss: 0.2755 - val_acc: 0.8746\n",
      "Epoch 10/100\n",
      "5600000/5600000 [==============================] - 47s - loss: 0.2762 - acc: 0.8738 - val_loss: 0.2758 - val_acc: 0.8744\n",
      "Epoch 11/100\n",
      "5600000/5600000 [==============================] - 47s - loss: 0.2756 - acc: 0.8743 - val_loss: 0.2749 - val_acc: 0.8749\n",
      "Epoch 12/100\n",
      "5600000/5600000 [==============================] - 48s - loss: 0.2748 - acc: 0.8748 - val_loss: 0.2743 - val_acc: 0.8751\n",
      "Epoch 13/100\n",
      "5600000/5600000 [==============================] - 48s - loss: 0.2740 - acc: 0.8753 - val_loss: 0.2765 - val_acc: 0.8741\n",
      "Epoch 14/100\n",
      "5600000/5600000 [==============================] - 47s - loss: 0.2733 - acc: 0.8757 - val_loss: 0.2723 - val_acc: 0.8763\n",
      "Epoch 15/100\n",
      "5600000/5600000 [==============================] - 47s - loss: 0.2726 - acc: 0.8762 - val_loss: 0.2712 - val_acc: 0.8770\n",
      "Epoch 16/100\n",
      "5600000/5600000 [==============================] - 46s - loss: 0.2720 - acc: 0.8766 - val_loss: 0.2720 - val_acc: 0.8763\n",
      "Epoch 17/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2715 - acc: 0.8769 - val_loss: 0.2729 - val_acc: 0.8765\n",
      "Epoch 18/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2711 - acc: 0.8772 - val_loss: 0.2701 - val_acc: 0.8777\n",
      "Epoch 19/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2707 - acc: 0.8773 - val_loss: 0.2707 - val_acc: 0.8772\n",
      "Epoch 20/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2704 - acc: 0.8776 - val_loss: 0.2698 - val_acc: 0.8780\n",
      "Epoch 21/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2702 - acc: 0.8777 - val_loss: 0.2698 - val_acc: 0.8780\n",
      "Epoch 22/100\n",
      "5600000/5600000 [==============================] - 48s - loss: 0.2699 - acc: 0.8780 - val_loss: 0.2696 - val_acc: 0.8783\n",
      "Epoch 23/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2695 - acc: 0.8781 - val_loss: 0.2688 - val_acc: 0.8786\n",
      "Epoch 24/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2693 - acc: 0.8782 - val_loss: 0.2689 - val_acc: 0.8784\n",
      "Epoch 25/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2691 - acc: 0.8783 - val_loss: 0.2689 - val_acc: 0.8788\n",
      "Epoch 26/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2689 - acc: 0.8785 - val_loss: 0.2688 - val_acc: 0.8785\n",
      "Epoch 27/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2685 - acc: 0.8787 - val_loss: 0.2680 - val_acc: 0.8790\n",
      "Epoch 28/100\n",
      "5600000/5600000 [==============================] - 45s - loss: 0.2684 - acc: 0.8788 - val_loss: 0.2675 - val_acc: 0.8793\n",
      "Epoch 29/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2681 - acc: 0.8789 - val_loss: 0.2673 - val_acc: 0.8796\n",
      "Epoch 30/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2679 - acc: 0.8791 - val_loss: 0.2673 - val_acc: 0.8796\n",
      "Epoch 31/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2677 - acc: 0.8791 - val_loss: 0.2673 - val_acc: 0.8796\n",
      "Epoch 32/100\n",
      "5600000/5600000 [==============================] - 44s - loss: 0.2675 - acc: 0.8792 - val_loss: 0.2675 - val_acc: 0.8792\n"
     ]
    }
   ],
   "source": [
    "depth_factor=3\n",
    "\n",
    "print('Fitting network with depth factor = {}...'.format(depth_factor))\n",
    "witm_network = create_witm(num_inputs=28,\n",
    "                              l2_val=L2_CONSTANT,\n",
    "                              width_factor=2,\n",
    "                              depth_factor=depth_factor,\n",
    "                              num_outputs=1,\n",
    "                              optimizer=sgd)\n",
    "print(witm_network.summary())\n",
    "    # fitting model\n",
    "history = witm_network.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), epochs=100, \n",
    "                 batch_size=1024, callbacks=[early_stopping])\n",
    "val_accs_witm.append(max(history.history['val_acc']))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
